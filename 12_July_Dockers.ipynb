{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version: '3'\n",
    "services:\n",
    "  web:\n",
    "    build: .\n",
    "    ports:\n",
    "      - 80:80\n",
    "    depends_on:\n",
    "      - database\n",
    "      - cache\n",
    "\n",
    "  database:\n",
    "    image: postgres:latest\n",
    "    environment:\n",
    "      - POSTGRES_USER=myuser\n",
    "      - POSTGRES_PASSWORD=mypassword\n",
    "\n",
    "  cache:\n",
    "    image: redis:latest\n",
    "\n",
    "\n",
    "\n",
    "    import docker\n",
    "import psutil\n",
    "\n",
    "def scale_containers(container_name, threshold, max_replicas):\n",
    "    client = docker.from_env()\n",
    "\n",
    "    while True:\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "\n",
    "        if cpu_percent >= threshold:\n",
    "            # Get the current number of replicas\n",
    "            container = client.containers.get(container_name)\n",
    "            current_replicas = container.attrs['Spec']['Mode']['Replicated']['Replicas']\n",
    "\n",
    "            # Scale up by 1 if not exceeding the maximum number of replicas\n",
    "            if current_replicas < max_replicas:\n",
    "                new_replicas = current_replicas + 1\n",
    "                print(f\"Scaling up to {new_replicas} replicas.\")\n",
    "                container.update(replicas=new_replicas)\n",
    "        else:\n",
    "            # Get the current number of replicas\n",
    "            container = client.containers.get(container_name)\n",
    "            current_replicas = container.attrs['Spec']['Mode']['Replicated']['Replicas']\n",
    "\n",
    "            # Scale down by 1 if more than 1 replica\n",
    "            if current_replicas > 1:\n",
    "                new_replicas = current_replicas - 1\n",
    "                print(f\"Scaling down to {new_replicas} replicas.\")\n",
    "                container.update(replicas=new_replicas)\n",
    "\n",
    "# Example usage\n",
    "container_name = \"my_container\"  # Replace with your container name\n",
    "threshold = 70  # CPU usage threshold in percentage\n",
    "max_replicas = 5  # Maximum number of replicas\n",
    "\n",
    "scale_containers(container_name, threshold, max_replicas)\n",
    "\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Variables\n",
    "registry_url=\"https://your-registry.com\"\n",
    "image_name=\"your-image\"\n",
    "container_name=\"your-container\"\n",
    "\n",
    "# Authenticate with the registry\n",
    "docker login $registry_url\n",
    "\n",
    "# Pull the latest version of the image\n",
    "docker pull $registry_url/$image_name\n",
    "\n",
    "# Run a container based on the image\n",
    "docker run -d --name $container_name $registry_url/$image_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'shell_command_dag',\n",
    "    start_date=datetime(2022, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "# Define the task\n",
    "execute_shell_command = BashOperator(\n",
    "    task_id='execute_shell_command',\n",
    "    bash_command='your_shell_command_here',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def process_input(input_value):\n",
    "    # Process the input value\n",
    "    print(f\"Processing input: {input_value}\")\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'dynamic_tasks_dag',\n",
    "    start_date=datetime(2022, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "# Define the list of inputs\n",
    "input_list = ['input1', 'input2', 'input3']\n",
    "\n",
    "# Generate tasks dynamically using PythonOperator\n",
    "tasks = []\n",
    "for input_value in input_list:\n",
    "    task = PythonOperator(\n",
    "        task_id=f'process_input_{input_value}',\n",
    "        python_callable=process_input,\n",
    "        op_kwargs={'input_value': input_value},\n",
    "        dag=dag\n",
    "    )\n",
    "    tasks.append(task)\n",
    "\n",
    "# Set task dependencies\n",
    "for i in range(1, len(tasks)):\n",
    "    tasks[i] >> tasks[i-1]\n",
    "\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.dagrun import TriggerDagRunOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'complex_dependency_dag',\n",
    "    start_date=datetime(2022, 1, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "# Define Task A\n",
    "task_a = TriggerDagRunOperator(\n",
    "    task_id='task_a',\n",
    "    trigger_dag_id='my_dag_id',\n",
    "    execution_date='{{ execution_date }}',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define Task B\n",
    "task_b = TriggerDagRunOperator(\n",
    "    task_id='task_b',\n",
    "    trigger_dag_id='my_dag_id',\n",
    "    execution_date='{{ execution_date }}',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Set the task dependency\n",
    "task_a >> task_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:oracle:thin:@//hostname:port/service \\\n",
    "--username your_username \\\n",
    "--password your_password \\\n",
    "--table your_table \\\n",
    "--columns \"column1,column2,column3\" \\\n",
    "--target-dir /path/to/hdfs/directory\n",
    "\n",
    "\n",
    "\n",
    "sqoop import \\\n",
    "--connect jdbc:mysql://hostname:port/database \\\n",
    "--username your_username \\\n",
    "--password your_password \\\n",
    "--table your_table \\\n",
    "--incremental append \\\n",
    "--check-column your_timestamp_column \\\n",
    "--last-value '2022-07-01 00:00:00' \\\n",
    "--merge-key your_primary_key \\\n",
    "--target-dir /path/to/hdfs/directory\n",
    "\n",
    "\n",
    "\n",
    "sqoop export \\\n",
    "--connect \"jdbc:sqlserver://hostname:port;databaseName=your_database\" \\\n",
    "--username your_username \\\n",
    "--password your_password \\\n",
    "--table your_table \\\n",
    "--export-dir /path/to/hdfs/directory \\\n",
    "--input-fields-terminated-by ',' \\\n",
    "--input-lines-terminated-by '\\n'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
