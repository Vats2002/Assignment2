{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "def read_hadoop_config(file_path):\n",
    "    # Create a ConfigParser object\n",
    "    config = ConfigParser()\n",
    "    \n",
    "    # Read the configuration file\n",
    "    config.read(file_path)\n",
    "    \n",
    "    # Get the core section\n",
    "    if config.has_section('core'):\n",
    "        core_components = config.options('core')\n",
    "        \n",
    "        # Display the core components\n",
    "        print(\"Core Components of Hadoop:\")\n",
    "        for component in core_components:\n",
    "            print(component)\n",
    "    else:\n",
    "        print(\"No core section found in the configuration file.\")\n",
    "\n",
    "# Provide the path to your Hadoop configuration file\n",
    "config_file_path = 'path/to/your/hadoop-config-file.conf'\n",
    "\n",
    "# Call the function to read and display the core components\n",
    "read_hadoop_config(config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "def calculate_directory_size(hdfs_host, hdfs_port, hdfs_directory):\n",
    "    # Create a connection to the HDFS\n",
    "    hdfs_client = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "    \n",
    "    # Get the file status of the HDFS directory\n",
    "    dir_status = hdfs_client.get_path_info(hdfs_directory)\n",
    "    \n",
    "    # Check if the path is a directory\n",
    "    if not dir_status['kind'] == 'directory':\n",
    "        print(f\"{hdfs_directory} is not a directory.\")\n",
    "        return\n",
    "    \n",
    "    # Recursive function to calculate the total size\n",
    "    def calculate_size(path):\n",
    "        size = 0\n",
    "        \n",
    "        # Get the status of the path\n",
    "        status = hdfs_client.get_path_info(path)\n",
    "        \n",
    "        # If it's a file, return its size\n",
    "        if status['kind'] == 'file':\n",
    "            return status['size']\n",
    "        \n",
    "        # If it's a directory, recursively calculate the size of its contents\n",
    "        if status['kind'] == 'directory':\n",
    "            for file in hdfs_client.ls(path):\n",
    "                file_path = path + '/' + file\n",
    "                size += calculate_size(file_path)\n",
    "        \n",
    "        return size\n",
    "    \n",
    "    # Call the recursive function to calculate the total size\n",
    "    total_size = calculate_size(hdfs_directory)\n",
    "    \n",
    "    # Print the total size\n",
    "    print(f\"Total size of {hdfs_directory}: {total_size} bytes\")\n",
    "\n",
    "# Set the HDFS host, port, and directory path\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9000\n",
    "hdfs_directory = '/path/to/hdfs/directory'\n",
    "\n",
    "# Call the function to calculate the total size\n",
    "calculate_directory_size(hdfs_host, hdfs_port, hdfs_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from heapq import nlargest\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(MRWordFrequencyCount, self).configure_args()\n",
    "        self.add_passthru_arg('-n', '--top_n', type=int, default=10, help='Specify the number of most frequent words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_words)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = WORD_RE.findall(line)\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_words(self, _, word_count_pairs):\n",
    "        top_n = self.options.top_n\n",
    "        top_words = nlargest(top_n, word_count_pairs)\n",
    "        for count, word in top_words:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_health(nn_host, nn_port):\n",
    "    url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            \n",
    "            if json_data['beans']:\n",
    "                live_nodes = json_data['beans'][0]['LiveNodes']\n",
    "                dead_nodes = json_data['beans'][0]['DeadNodes']\n",
    "                \n",
    "                print(f\"NameNode Health Status:\")\n",
    "                print(f\"Live Nodes: {live_nodes}\")\n",
    "                print(f\"Dead Nodes: {dead_nodes}\")\n",
    "            else:\n",
    "                print(\"No data available for NameNode health check.\")\n",
    "        else:\n",
    "            print(f\"Error accessing NameNode: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing NameNode: {e}\")\n",
    "\n",
    "def check_datanode_health(nn_host, nn_port):\n",
    "    url = f\"http://{nn_host}:{nn_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            \n",
    "            if json_data['beans']:\n",
    "                live_nodes = json_data['beans'][0]['LiveNodes']\n",
    "                dead_nodes = json_data['beans'][0]['DeadNodes']\n",
    "                \n",
    "                print(f\"DataNode Health Status:\")\n",
    "                print(f\"Live Nodes: {live_nodes}\")\n",
    "                print(f\"Dead Nodes: {dead_nodes}\")\n",
    "            else:\n",
    "                print(\"No data available for DataNode health check.\")\n",
    "        else:\n",
    "            print(f\"Error accessing DataNode: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing DataNode: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the NameNode host and port\n",
    "    nn_host = 'localhost'\n",
    "    nn_port = 50070\n",
    "    \n",
    "    # Check NameNode health status\n",
    "    check_namenode_health(nn_host, nn_port)\n",
    "    \n",
    "    # Check DataNode health status\n",
    "    check_datanode_health(nn_host, nn_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.hdfs as hdfs\n",
    "\n",
    "def list_hdfs_path(hdfs_host, hdfs_port, hdfs_path):\n",
    "    # Create a connection to the HDFS\n",
    "    hdfs_client = hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
    "    \n",
    "    # List the files and directories in the HDFS path\n",
    "    files = hdfs_client.ls(hdfs_path)\n",
    "    \n",
    "    # Display the files and directories\n",
    "    print(f\"Files and directories in {hdfs_path}:\")\n",
    "    for file in files:\n",
    "        print(file)\n",
    "\n",
    "# Set the HDFS host, port, and path\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9000\n",
    "hdfs_path = '/path/to/hdfs/directory'\n",
    "\n",
    "# Call the function to list the files and directories\n",
    "list_hdfs_path(hdfs_host, hdfs_port, hdfs_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_storage_utilization(hdfs_host, hdfs_port):\n",
    "    url = f\"http://{hdfs_host}:{hdfs_port}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            \n",
    "            if json_data['beans']:\n",
    "                data_nodes = json_data['beans'][0]['Storage']\n",
    "                \n",
    "                # Calculate storage utilization for each DataNode\n",
    "                storage_utilization = []\n",
    "                for data_node in data_nodes:\n",
    "                    node_name = data_node['DataNodeName']\n",
    "                    capacity = data_node['Capacity']\n",
    "                    dfs_used = data_node['DfsUsed']\n",
    "                    utilization_percentage = (dfs_used / capacity) * 100\n",
    "                    \n",
    "                    storage_utilization.append((node_name, utilization_percentage))\n",
    "                \n",
    "                # Sort DataNodes by storage utilization\n",
    "                sorted_utilization = sorted(storage_utilization, key=lambda x: x[1])\n",
    "                \n",
    "                # Print DataNodes with highest and lowest storage capacities\n",
    "                print(\"DataNodes Storage Utilization:\")\n",
    "                print(f\"Highest Utilization: {sorted_utilization[-1][0]} - {sorted_utilization[-1][1]:.2f}%\")\n",
    "                print(f\"Lowest Utilization: {sorted_utilization[0][0]} - {sorted_utilization[0][1]:.2f}%\")\n",
    "            else:\n",
    "                print(\"No data available for DataNode storage utilization analysis.\")\n",
    "        else:\n",
    "            print(f\"Error accessing DataNode: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing DataNode: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the HDFS host and port\n",
    "    hdfs_host = 'localhost'\n",
    "    hdfs_port = 50070\n",
    "    \n",
    "    # Analyze storage utilization\n",
    "    analyze_storage_utilization(hdfs_host, hdfs_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/new-application\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            application_id = json_data['application-id']\n",
    "            \n",
    "            # Submit the Hadoop job\n",
    "            submit_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            data = {\n",
    "                \"application-id\": application_id,\n",
    "                \"application-name\": \"MyHadoopJob\",\n",
    "                \"am-container-spec\": {\n",
    "                    \"commands\": {\n",
    "                        \"command\": f\"hadoop jar {job_file}\"\n",
    "                    },\n",
    "                    \"resource\": {\n",
    "                        \"memory\": 1024,\n",
    "                        \"vCores\": 1\n",
    "                    }\n",
    "                },\n",
    "                \"unmanaged-AM\": False,\n",
    "                \"max-app-attempts\": 2\n",
    "            }\n",
    "            \n",
    "            response = requests.post(submit_url, headers=headers, json=data)\n",
    "            if response.status_code == 202:\n",
    "                print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "                return application_id\n",
    "            else:\n",
    "                print(f\"Error submitting Hadoop job: {response.status_code}\")\n",
    "        else:\n",
    "            print(f\"Error creating new application: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "def monitor_job_progress(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                state = json_data['app']['state']\n",
    "                final_status = json_data['app']['finalStatus']\n",
    "                \n",
    "                print(f\"Job State: {state}\")\n",
    "                \n",
    "                if final_status != \"UNDEFINED\":\n",
    "                    print(f\"Job Final Status: {final_status}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error getting job progress: {response.status_code}\")\n",
    "            \n",
    "            time.sleep(5)  # Wait for 5 seconds before checking progress again\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "def retrieve_job_output(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/proxy/{application_id}/ws/v1/mapreduce/jobs\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            if 'jobs' in json_data and 'job' in json_data['jobs']:\n",
    "                job_id = json_data['jobs']['job'][0]['id']\n",
    "                job_output_url = f\"{url}/{job_id}/jobattempts\"\n",
    "                \n",
    "                response = requests.get(job_output_url)\n",
    "                if response.status_code == 200:\n",
    "                    json_data = response.json()\n",
    "                    if 'jobAttempts' in json_data and 'jobAttempt' in json_data['jobAttempts']:\n",
    "                        latest_attempt = json_data['jobAttempts']['jobAttempt'][0]['id']\n",
    "                        output_url = f\"{job_output_url}/{latest_attempt}/logs\"\n",
    "                        \n",
    "                        response = requests.get(output_url)\n",
    "                        if response.status_code == 200:\n",
    "                            job_output = response.text\n",
    "                            print(f\"Job Output:\\n{job_output}\")\n",
    "                        else:\n",
    "                            print(f\"Error retrieving job output: {response.status_code}\")\n",
    "                    else:\n",
    "                        print(\"No job attempts found.\")\n",
    "                else:\n",
    "                    print(f\"Error retrieving job attempts: {response.status_code}\")\n",
    "            else:\n",
    "                print(\"No job found.\")\n",
    "        else:\n",
    "            print(f\"Error retrieving job details: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the ResourceManager host and port\n",
    "    resourcemanager_host = 'localhost'\n",
    "    resourcemanager_port = 8088\n",
    "    \n",
    "    # Set the path to the Hadoop job JAR file\n",
    "    job_file = '/path/to/hadoop/job.jar'\n",
    "    \n",
    "    # Submit the Hadoop job\n",
    "    application_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file)\n",
    "    \n",
    "    if application_id:\n",
    "        # Monitor job progress\n",
    "        monitor_job_progress(resourcemanager_host, resourcemanager_port, application_id)\n",
    "        \n",
    "        # Retrieve job output\n",
    "        retrieve_job_output(resourcemanager_host, resourcemanager_port, application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file, job_args):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/new-application\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            application_id = json_data['application-id']\n",
    "            \n",
    "            # Submit the Hadoop job\n",
    "            submit_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            data = {\n",
    "                \"application-id\": application_id,\n",
    "                \"application-name\": \"MyHadoopJob\",\n",
    "                \"am-container-spec\": {\n",
    "                    \"commands\": {\n",
    "                        \"command\": f\"hadoop jar {job_file} {' '.join(job_args)}\"\n",
    "                    },\n",
    "                    \"resource\": {\n",
    "                        \"memory\": 1024,\n",
    "                        \"vCores\": 1\n",
    "                    }\n",
    "                },\n",
    "                \"unmanaged-AM\": False,\n",
    "                \"max-app-attempts\": 2\n",
    "            }\n",
    "            \n",
    "            response = requests.post(submit_url, headers=headers, json=data)\n",
    "            if response.status_code == 202:\n",
    "                print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "                return application_id\n",
    "            else:\n",
    "                print(f\"Error submitting Hadoop job: {response.status_code}\")\n",
    "        else:\n",
    "            print(f\"Error creating new application: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "def track_resource_usage(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                state = json_data['app']['state']\n",
    "                final_status = json_data['app']['finalStatus']\n",
    "                allocated_resources = json_data['app']['allocatedResources']\n",
    "                \n",
    "                print(f\"Job State: {state}\")\n",
    "                print(f\"Job Final Status: {final_status}\")\n",
    "                print(f\"Allocated Resources: {allocated_resources}\")\n",
    "                \n",
    "                if final_status != \"UNDEFINED\":\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error getting job progress: {response.status_code}\")\n",
    "            \n",
    "            time.sleep(5)  # Wait for 5 seconds before checking progress again\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the ResourceManager host and port\n",
    "    resourcemanager_host = 'localhost'\n",
    "    resourcemanager_port = 8088\n",
    "    \n",
    "    # Set the path to the Hadoop job JAR file and job arguments\n",
    "    job_file = '/path/to/hadoop/job.jar'\n",
    "    job_args = ['arg1', 'arg2']\n",
    "    \n",
    "    # Submit the Hadoop job\n",
    "    application_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file, job_args)\n",
    "    \n",
    "    if application_id:\n",
    "        # Track resource usage\n",
    "        track_resource_usage(resourcemanager_host, resourcemanager_port, application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file, job_args):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/new-application\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url)\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            application_id = json_data['application-id']\n",
    "            \n",
    "            # Submit the Hadoop job\n",
    "            submit_url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            data = {\n",
    "                \"application-id\": application_id,\n",
    "                \"application-name\": \"MyHadoopJob\",\n",
    "                \"am-container-spec\": {\n",
    "                    \"commands\": {\n",
    "                        \"command\": f\"hadoop jar {job_file} {' '.join(job_args)}\"\n",
    "                    },\n",
    "                    \"resource\": {\n",
    "                        \"memory\": 1024,\n",
    "                        \"vCores\": 1\n",
    "                    }\n",
    "                },\n",
    "                \"unmanaged-AM\": False,\n",
    "                \"max-app-attempts\": 2\n",
    "            }\n",
    "            \n",
    "            response = requests.post(submit_url, headers=headers, json=data)\n",
    "            if response.status_code == 202:\n",
    "                print(f\"Hadoop job submitted. Application ID: {application_id}\")\n",
    "                return application_id\n",
    "            else:\n",
    "                print(f\"Error submitting Hadoop job: {response.status_code}\")\n",
    "        else:\n",
    "            print(f\"Error creating new application: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "def track_resource_usage(resourcemanager_host, resourcemanager_port, application_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{application_id}\"\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                state = json_data['app']['state']\n",
    "                final_status = json_data['app']['finalStatus']\n",
    "                allocated_resources = json_data['app']['allocatedResources']\n",
    "                \n",
    "                print(f\"Job State: {state}\")\n",
    "                print(f\"Job Final Status: {final_status}\")\n",
    "                print(f\"Allocated Resources: {allocated_resources}\")\n",
    "                \n",
    "                if final_status != \"UNDEFINED\":\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error getting job progress: {response.status_code}\")\n",
    "            \n",
    "            time.sleep(5)  # Wait for 5 seconds before checking progress again\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error interacting with ResourceManager: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the ResourceManager host and port\n",
    "    resourcemanager_host = 'localhost'\n",
    "    resourcemanager_port = 8088\n",
    "    \n",
    "    # Set the path to the Hadoop job JAR file and job arguments\n",
    "    job_file = '/path/to/hadoop/job.jar'\n",
    "    job_args = ['arg1', 'arg2']\n",
    "    \n",
    "    # Submit the Hadoop job\n",
    "    application_id = submit_hadoop_job(resourcemanager_host, resourcemanager_port, job_file, job_args)\n",
    "    \n",
    "    if application_id:\n",
    "        # Track resource usage\n",
    "        track_resource_usage(resourcemanager_host, resourcemanager_port, application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(MRWordCount, self).configure_args()\n",
    "        self.add_passthru_arg('-s', '--split_size', type=int, default=100, help='Specify the input split size')\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "    \n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def job_runner_kwargs(self):\n",
    "        kwargs = super(MRWordCount, self).job_runner_kwargs()\n",
    "        split_size = self.options.split_size\n",
    "        kwargs['jobconf'] = {'mapreduce.input.fileinputformat.split.maxsize': str(split_size)}\n",
    "        return kwargs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the input file path\n",
    "    input_file = '/path/to/your/input_file.txt'\n",
    "    \n",
    "    # Set the different input split sizes to compare\n",
    "    split_sizes = [100, 500, 1000]\n",
    "    \n",
    "    for split_size in split_sizes:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the MRJob with the specified split size\n",
    "        MRWordCount(args=[input_file, '-s', str(split_size)]).run_job()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Execution time for split size {split_size}: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
