{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def read_hadoop_config(file_path):\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # Read the configuration file\n",
    "    config.read(file_path)\n",
    "\n",
    "    # Get the core section\n",
    "    if 'core-site' in config:\n",
    "        core_section = config['core-site']\n",
    "        \n",
    "        # Display the core components of Hadoop\n",
    "        if 'fs.defaultFS' in core_section:\n",
    "            print('File System: ', core_section['fs.defaultFS'])\n",
    "        if 'hadoop.tmp.dir' in core_section:\n",
    "            print('Temporary Directory: ', core_section['hadoop.tmp.dir'])\n",
    "        # Add more components as per your requirement\n",
    "\n",
    "    else:\n",
    "        print('No core-site section found in the configuration file.')\n",
    "\n",
    "# Provide the path to your Hadoop configuration file\n",
    "config_file_path = 'path/to/hadoop/conf/core-site.xml'\n",
    "\n",
    "# Call the function to read and display the core components of Hadoop\n",
    "read_hadoop_config(config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def calculate_total_file_size(directory_path, hdfs_host, hdfs_port, hdfs_user):\n",
    "    # Create a PyWebHdfsClient object\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "    # Retrieve file information for the directory\n",
    "    file_info = hdfs.list_dir(directory_path)['FileStatuses']['FileStatus']\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    # Iterate through the files in the directory\n",
    "    for file in file_info:\n",
    "        if file['type'] == 'FILE':\n",
    "            # Add the file size to the total\n",
    "            total_size += file['length']\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# Provide the HDFS directory path, host, port, and user\n",
    "directory_path = '/path/to/hdfs/directory'\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 50070\n",
    "hdfs_user = 'hadoop'\n",
    "\n",
    "# Call the function to calculate the total file size\n",
    "total_size = calculate_total_file_size(directory_path, hdfs_host, hdfs_port, hdfs_user)\n",
    "\n",
    "print(\"Total File Size:\", total_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_REGEX = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('-n', '--topN', type=int, default=10, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_topN)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        words = WORD_REGEX.findall(line)\n",
    "        for word in words:\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_topN(self, _, word_counts):\n",
    "        topN = self.options.topN\n",
    "        sorted_word_counts = sorted(word_counts, reverse=True)\n",
    "        for i in range(topN):\n",
    "            if i < len(sorted_word_counts):\n",
    "                count, word = sorted_word_counts[i]\n",
    "                yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the Namenode and ResourceManager REST API endpoints\n",
    "namenode_url = 'http://<namenode_host>:<namenode_port>/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "datanode_url = 'http://<datanode_host>:<datanode_port>/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "\n",
    "def check_namenode_health():\n",
    "    response = requests.get(namenode_url)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        live_nodes = json_data['beans'][0]['LiveNodes']\n",
    "        if live_nodes:\n",
    "            print(\"Namenode is healthy. Live Nodes: \", live_nodes)\n",
    "        else:\n",
    "            print(\"Namenode is unhealthy. No Live Nodes found.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve Namenode health information.\")\n",
    "\n",
    "def check_datanode_health():\n",
    "    response = requests.get(datanode_url)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        num_live_data_nodes = json_data['beans'][0]['NumLiveDataNodes']\n",
    "        if num_live_data_nodes > 0:\n",
    "            print(\"DataNodes are healthy. Number of Live DataNodes: \", num_live_data_nodes)\n",
    "        else:\n",
    "            print(\"DataNodes are unhealthy. No Live DataNodes found.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve DataNode health information.\")\n",
    "\n",
    "# Call the functions to check the health status\n",
    "check_namenode_health()\n",
    "check_datanode_health()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def list_hdfs_path(hdfs_path, hdfs_host, hdfs_port, hdfs_user):\n",
    "    # Create a PyWebHdfsClient object\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "    # List the files and directories in the HDFS path\n",
    "    response = hdfs.list_dir(hdfs_path)\n",
    "\n",
    "    # Iterate through the file statuses and print their names\n",
    "    for file_status in response['FileStatuses']['FileStatus']:\n",
    "        name = file_status['pathSuffix']\n",
    "        if file_status['type'] == 'DIRECTORY':\n",
    "            name += '/'\n",
    "        print(name)\n",
    "\n",
    "# Provide the HDFS path, host, port, and user\n",
    "hdfs_path = '/path/to/hdfs/directory'\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 50070\n",
    "hdfs_user = 'hadoop'\n",
    "\n",
    "# Call the function to list the files and directories\n",
    "list_hdfs_path(hdfs_path, hdfs_host, hdfs_port, hdfs_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the DataNode REST API endpoint\n",
    "datanode_url = 'http://<datanode_host>:<datanode_port>/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState'\n",
    "\n",
    "def analyze_storage_utilization():\n",
    "    response = requests.get(datanode_url)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        datanodes = json_data['beans'][0]['StorageInfo']['0']['DataNodeVolumeInfo']\n",
    "        if datanodes:\n",
    "            # Sort DataNodes based on storage utilization\n",
    "            sorted_datanodes = sorted(datanodes, key=lambda x: x['usedSpace'], reverse=True)\n",
    "            \n",
    "            # Print DataNode with highest storage capacity\n",
    "            highest_datanode = sorted_datanodes[0]\n",
    "            print(\"DataNode with highest storage capacity:\")\n",
    "            print(\"Host: \", highest_datanode['hostName'])\n",
    "            print(\"Storage Capacity: \", highest_datanode['capacity'])\n",
    "            print(\"Used Space: \", highest_datanode['usedSpace'])\n",
    "            print()\n",
    "            \n",
    "            # Print DataNode with lowest storage capacity\n",
    "            lowest_datanode = sorted_datanodes[-1]\n",
    "            print(\"DataNode with lowest storage capacity:\")\n",
    "            print(\"Host: \", lowest_datanode['hostName'])\n",
    "            print(\"Storage Capacity: \", lowest_datanode['capacity'])\n",
    "            print(\"Used Space: \", lowest_datanode['usedSpace'])\n",
    "        else:\n",
    "            print(\"No DataNodes found.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve DataNode information.\")\n",
    "\n",
    "# Call the function to analyze storage utilization\n",
    "analyze_storage_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the ResourceManager REST API endpoint\n",
    "resource_manager_url = 'http://<resource_manager_host>:<resource_manager_port>/ws/v1/cluster'\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path):\n",
    "    # Create the Hadoop job payload\n",
    "    payload = {\n",
    "        'application-id': '',\n",
    "        'application-name': 'Hadoop Job',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {} {}'.format(jar_path, main_class, input_path, output_path)\n",
    "            },\n",
    "            'local-resources': {},\n",
    "            'environment': {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Submit the Hadoop job\n",
    "    response = requests.post(resource_manager_url + '/apps', json=payload)\n",
    "    if response.status_code == 202:\n",
    "        application_id = response.json()['application-id']\n",
    "        print(\"Hadoop job submitted successfully. Application ID: \", application_id)\n",
    "        return application_id\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return None\n",
    "\n",
    "def monitor_job_progress(application_id):\n",
    "    while True:\n",
    "        # Retrieve job status\n",
    "        response = requests.get(resource_manager_url + '/apps/' + application_id)\n",
    "        if response.status_code == 200:\n",
    "            status = response.json()['app']['state']\n",
    "            print(\"Job status: \", status)\n",
    "            \n",
    "            # Check if the job is completed\n",
    "            if status == 'FINISHED':\n",
    "                break\n",
    "            elif status in ['FAILED', 'KILLED']:\n",
    "                print(\"Job execution failed or was killed.\")\n",
    "                return\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to retrieve job status.\")\n",
    "            return\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "\n",
    "def retrieve_job_output(application_id):\n",
    "    # Retrieve the job output\n",
    "    response = requests.get(resource_manager_url + '/apps/' + application_id + '/state')\n",
    "    if response.status_code == 200:\n",
    "        final_output = response.json()['app']['finalStatus']\n",
    "        print(\"Job output: \", final_output)\n",
    "    else:\n",
    "        print(\"Failed to retrieve job output.\")\n",
    "\n",
    "# Specify the jar path, main class, input path, and output path\n",
    "jar_path = '/path/to/your/hadoop/job.jar'\n",
    "main_class = 'com.example.YourMainClass'\n",
    "input_path = '/path/to/your/input'\n",
    "output_path = '/path/to/your/output'\n",
    "\n",
    "# Submit the Hadoop job and retrieve the application ID\n",
    "application_id = submit_hadoop_job(jar_path, main_class, input_path, output_path)\n",
    "\n",
    "if application_id:\n",
    "    # Monitor job progress\n",
    "    monitor_job_progress(application_id)\n",
    "\n",
    "    # Retrieve job output\n",
    "    retrieve_job_output(application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Specify the ResourceManager REST API endpoint\n",
    "resource_manager_url = 'http://<resource_manager_host>:<resource_manager_port>/ws/v1/cluster'\n",
    "\n",
    "def submit_hadoop_job(jar_path, main_class, input_path, output_path, memory_mb, vcores):\n",
    "    # Create the Hadoop job payload\n",
    "    payload = {\n",
    "        'application-id': '',\n",
    "        'application-name': 'Hadoop Job',\n",
    "        'am-container-spec': {\n",
    "            'commands': {\n",
    "                'command': 'hadoop jar {} {} {} {}'.format(jar_path, main_class, input_path, output_path)\n",
    "            },\n",
    "            'local-resources': {},\n",
    "            'environment': {}\n",
    "        },\n",
    "        'resource': {\n",
    "            'memory': memory_mb,\n",
    "            'vCores': vcores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Submit the Hadoop job\n",
    "    response = requests.post(resource_manager_url + '/apps', json=payload)\n",
    "    if response.status_code == 202:\n",
    "        application_id = response.json()['application-id']\n",
    "        print(\"Hadoop job submitted successfully. Application ID: \", application_id)\n",
    "        return application_id\n",
    "    else:\n",
    "        print(\"Failed to submit the Hadoop job.\")\n",
    "        return None\n",
    "\n",
    "def monitor_resource_usage(application_id):\n",
    "    while True:\n",
    "        # Retrieve application statistics\n",
    "        response = requests.get(resource_manager_url + '/apps/' + application_id + '/statistics')\n",
    "        if response.status_code == 200:\n",
    "            statistics = response.json()['appStatInfo']['resourceInfo']\n",
    "            memory_mb = statistics['memoryMB']\n",
    "            vcores = statistics['vCores']\n",
    "            print(\"Memory Usage: \", memory_mb, \" MB\")\n",
    "            print(\"vCores Usage: \", vcores)\n",
    "        else:\n",
    "            print(\"Failed to retrieve resource usage statistics.\")\n",
    "            return\n",
    "\n",
    "        # Check if the job is completed\n",
    "        response = requests.get(resource_manager_url + '/apps/' + application_id)\n",
    "        if response.status_code == 200:\n",
    "            status = response.json()['app']['state']\n",
    "            print(\"Job status: \", status)\n",
    "            if status == 'FINISHED':\n",
    "                break\n",
    "            elif status in ['FAILED', 'KILLED']:\n",
    "                print(\"Job execution failed or was killed.\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Failed to retrieve job status.\")\n",
    "            return\n",
    "\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "\n",
    "# Specify the jar path, main class, input path, output path, memory, and vcores\n",
    "jar_path = '/path/to/your/hadoop/job.jar'\n",
    "main_class = 'com.example.YourMainClass'\n",
    "input_path = '/path/to/your/input'\n",
    "output_path = '/path/to/your/output'\n",
    "memory_mb = 1024\n",
    "vcores = 2\n",
    "\n",
    "# Submit the Hadoop job and retrieve the application ID\n",
    "application_id = submit_hadoop_job(jar_path, main_class, input_path, output_path, memory_mb, vcores)\n",
    "\n",
    "if application_id:\n",
    "    # Monitor resource usage during job execution\n",
    "    monitor_resource_usage(application_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class PerformanceComparisonJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(PerformanceComparisonJob, self).configure_args()\n",
    "        self.add_passthru_arg('-s', '--split-size', type=int, default=64, help='Input split size in MB')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Map function implementation\n",
    "        # ...\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        # Reduce function implementation\n",
    "        # ...\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def mapper_final(self):\n",
    "        split_size_mb = self.options.split_size\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        self.increment_counter('Execution Time', f'Split Size: {split_size_mb}MB', elapsed_time)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PerformanceComparisonJob.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
